"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[1465],{4745(e,n,s){s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"architecture/data-flow","title":"Data Flow","description":"How messages flow through the STELLA system","source":"@site/docs/architecture/data-flow.md","sourceDirName":"architecture","slug":"/architecture/data-flow","permalink":"/STELLA_backend/docs/architecture/data-flow","draft":false,"unlisted":false,"editUrl":"https://github.com/c4dhi/STELLA_backend/tree/main/docs-site/docs/architecture/data-flow.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Data Flow","description":"How messages flow through the STELLA system"},"sidebar":"docsSidebar","previous":{"title":"Architecture Overview","permalink":"/STELLA_backend/docs/architecture/overview"},"next":{"title":"Session Lifecycle","permalink":"/STELLA_backend/docs/architecture/session-lifecycle"}}');var i=s(4848),r=s(8453);const a={sidebar_position:2,title:"Data Flow",description:"How messages flow through the STELLA system"},l="Data Flow",o={},c=[{value:"Voice Conversation Flow",id:"voice-conversation-flow",level:2},{value:"Step-by-Step Breakdown",id:"step-by-step-breakdown",level:2},{value:"1. User Speaks",id:"1-user-speaks",level:3},{value:"2. WebRTC Transport",id:"2-webrtc-transport",level:3},{value:"3. LiveKit Routing",id:"3-livekit-routing",level:3},{value:"4. Speech-to-Text (STT)",id:"4-speech-to-text-stt",level:3},{value:"5. LLM Processing",id:"5-llm-processing",level:3},{value:"6. Tool Execution (Optional)",id:"6-tool-execution-optional",level:3},{value:"7. Text-to-Speech (TTS)",id:"7-text-to-speech-tts",level:3},{value:"8. Audio Delivery",id:"8-audio-delivery",level:3},{value:"Data Channel Messages",id:"data-channel-messages",level:2},{value:"Message Types",id:"message-types",level:3},{value:"Flow Diagram",id:"flow-diagram",level:3},{value:"Database Persistence",id:"database-persistence",level:2},{value:"Write Path",id:"write-path",level:3},{value:"Read Path",id:"read-path",level:3},{value:"Latency Considerations",id:"latency-considerations",level:2},{value:"Latency Optimization",id:"latency-optimization",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"data-flow",children:"Data Flow"})}),"\n",(0,i.jsx)(n.p,{children:"Understanding how data flows through STELLA helps when debugging issues or extending the system. This document traces the path of a user's voice input from the browser to the AI response."}),"\n",(0,i.jsx)(n.h2,{id:"voice-conversation-flow",children:"Voice Conversation Flow"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  User    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Browser \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 LiveKit \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Agent \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 LLM \u2502\n\u2502 (Voice)  \u2502     \u2502 (WebRTC)\u2502     \u2502 Server  \u2502     \u2502 (STT) \u2502     \u2502     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n                                                     \u2502            \u2502\n                                                     \u2502            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  User    \u2502\u25c0\u2500\u2500\u2500\u2500\u2502 Browser \u2502\u25c0\u2500\u2500\u2500\u2500\u2502 LiveKit \u2502\u25c0\u2500\u2500\u2500\u2500\u2502 Agent \u2502\u25c0\u2500\u2500\u2500\u2500\u2502 LLM \u2502\n\u2502 (Hears)  \u2502     \u2502 (Audio) \u2502     \u2502 Server  \u2502     \u2502 (TTS) \u2502     \u2502     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-by-step-breakdown",children:"Step-by-Step Breakdown"}),"\n",(0,i.jsx)(n.h3,{id:"1-user-speaks",children:"1. User Speaks"}),"\n",(0,i.jsx)(n.p,{children:"The user speaks into their microphone. The browser captures the audio using the Web Audio API."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Browser captures audio\nnavigator.mediaDevices.getUserMedia({ audio: true })\n  .then(stream => {\n    // Stream is connected to LiveKit\n    room.localParticipant.publishTrack(stream.getAudioTracks()[0]);\n  });\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-webrtc-transport",children:"2. WebRTC Transport"}),"\n",(0,i.jsx)(n.p,{children:"The browser encodes the audio using Opus codec and sends it via WebRTC to the LiveKit server."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key characteristics:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Low latency (50-150ms typical)"}),"\n",(0,i.jsx)(n.li,{children:"Adaptive bitrate"}),"\n",(0,i.jsx)(n.li,{children:"Encrypted transport (DTLS-SRTP)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-livekit-routing",children:"3. LiveKit Routing"}),"\n",(0,i.jsx)(n.p,{children:"LiveKit receives the audio and routes it to all participants in the room, including the agent pod."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"User Audio \u2500\u2500\u25b6 LiveKit \u2500\u2500\u25b6 Agent Pod\n                 \u2502\n                 \u2514\u2500\u2500\u25b6 Other Participants (if any)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-speech-to-text-stt",children:"4. Speech-to-Text (STT)"}),"\n",(0,i.jsx)(n.p,{children:"The agent receives the audio stream and processes it through the STT engine."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Agent receives audio from LiveKit\nasync def on_audio_frame(self, frame: AudioFrame):\n    # Process through STT pipeline\n    text = await self.pipeline.transcribe(frame)\n\n    if text.is_final:\n        await self.on_transcript(text.text, is_final=True)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"STT Options:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sherpa-ONNX"}),": Local, low-latency, runs on CPU"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Whisper"}),": Higher accuracy, can run locally or via API"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud STT"}),": Google, Azure, AWS speech services"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"5-llm-processing",children:"5. LLM Processing"}),"\n",(0,i.jsx)(n.p,{children:"The transcribed text is sent to the LLM along with conversation history and system context."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def generate_response(self, user_input: str) -> str:\n    messages = [\n        {"role": "system", "content": self.system_prompt},\n        *self.conversation_history,\n        {"role": "user", "content": user_input}\n    ]\n\n    response = await self.openai.chat.completions.create(\n        model="gpt-4o",\n        messages=messages,\n        tools=self.get_tool_definitions()\n    )\n\n    return response.choices[0].message.content\n'})}),"\n",(0,i.jsx)(n.h3,{id:"6-tool-execution-optional",children:"6. Tool Execution (Optional)"}),"\n",(0,i.jsx)(n.p,{children:"If the LLM wants to use a tool, the agent executes it and feeds results back to the LLM."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# LLM requests a tool call\ntool_call = response.choices[0].message.tool_calls[0]\n\n# Agent executes the tool\nresult = await self.execute_tool(\n    tool_call.function.name,\n    json.loads(tool_call.function.arguments)\n)\n\n# Feed result back to LLM for final response\n"})}),"\n",(0,i.jsx)(n.h3,{id:"7-text-to-speech-tts",children:"7. Text-to-Speech (TTS)"}),"\n",(0,i.jsx)(n.p,{children:"The LLM's response is converted to audio through the TTS engine."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"async def speak(self, text: str):\n    # Stream TTS output\n    async for audio_chunk in self.pipeline.text_to_speech_stream(text):\n        # Publish to LiveKit room\n        await self.publish_audio(audio_chunk)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"TTS Options:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Kokoro"}),": Local, fast, good quality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Piper"}),": Local, multiple voices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ElevenLabs"}),": Cloud, very natural voices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenAI TTS"}),": Cloud, simple integration"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"8-audio-delivery",children:"8. Audio Delivery"}),"\n",(0,i.jsx)(n.p,{children:"The TTS audio is published to LiveKit and routed back to the user's browser."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Agent TTS Audio \u2500\u2500\u25b6 LiveKit \u2500\u2500\u25b6 User's Browser \u2500\u2500\u25b6 Speakers\n"})}),"\n",(0,i.jsx)(n.h2,{id:"data-channel-messages",children:"Data Channel Messages"}),"\n",(0,i.jsx)(n.p,{children:"In addition to audio, STELLA uses LiveKit data channels for text-based communication."}),"\n",(0,i.jsx)(n.h3,{id:"message-types",children:"Message Types"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:'// Transcript message (interim and final)\n{\n  type: "transcript",\n  speaker: "user" | "assistant",\n  text: "Hello, how can I help?",\n  isFinal: true,\n  timestamp: 1699876543210\n}\n\n// Status update\n{\n  type: "status",\n  status: "thinking" | "speaking" | "listening",\n  message: "Searching the database..."\n}\n\n// Progress update\n{\n  type: "progress",\n  todos: [\n    { id: "1", description: "Search database", status: "completed" },\n    { id: "2", description: "Generate response", status: "in_progress" }\n  ]\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"flow-diagram",children:"Flow Diagram"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Frontend \u2502                      \u2502  Agent   \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502                                 \u2502\n     \u2502\u2500\u2500\u2500\u2500 User types message \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502\n     \u2502                                 \u2502\n     \u2502\u25c0\u2500\u2500\u2500 status: "thinking" \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n     \u2502                                 \u2502\n     \u2502\u25c0\u2500\u2500\u2500 transcript (interim) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n     \u2502                                 \u2502\n     \u2502\u25c0\u2500\u2500\u2500 transcript (final) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n     \u2502                                 \u2502\n     \u2502\u25c0\u2500\u2500\u2500 status: "listening" \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n     \u2502                                 \u2502\n'})}),"\n",(0,i.jsx)(n.h2,{id:"database-persistence",children:"Database Persistence"}),"\n",(0,i.jsx)(n.p,{children:"All messages are persisted to PostgreSQL for history and analytics."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Messages table structure\nCREATE TABLE messages (\n  id            UUID PRIMARY KEY,\n  session_id    UUID REFERENCES sessions(id),\n  speaker       VARCHAR(20),  -- 'user' or 'assistant'\n  content       TEXT,\n  timestamp     TIMESTAMP WITH TIME ZONE,\n  metadata      JSONB\n);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"write-path",children:"Write Path"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Agent generates response"}),"\n",(0,i.jsx)(n.li,{children:"Backend receives message via WebSocket"}),"\n",(0,i.jsx)(n.li,{children:"Message saved to PostgreSQL"}),"\n",(0,i.jsx)(n.li,{children:"Message broadcast to all connected clients"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"read-path",children:"Read Path"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Client requests session history"}),"\n",(0,i.jsx)(n.li,{children:"Backend queries PostgreSQL"}),"\n",(0,i.jsx)(n.li,{children:"Messages returned in chronological order"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"latency-considerations",children:"Latency Considerations"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Stage"}),(0,i.jsx)(n.th,{children:"Typical Latency"}),(0,i.jsx)(n.th,{children:"Notes"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Audio Capture"}),(0,i.jsx)(n.td,{children:"~10ms"}),(0,i.jsx)(n.td,{children:"Browser processing"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"WebRTC Transport"}),(0,i.jsx)(n.td,{children:"50-150ms"}),(0,i.jsx)(n.td,{children:"Network dependent"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"STT Processing"}),(0,i.jsx)(n.td,{children:"100-500ms"}),(0,i.jsx)(n.td,{children:"Model and hardware dependent"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"LLM Generation"}),(0,i.jsx)(n.td,{children:"500-3000ms"}),(0,i.jsx)(n.td,{children:"Model and prompt dependent"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"TTS Generation"}),(0,i.jsx)(n.td,{children:"100-500ms"}),(0,i.jsx)(n.td,{children:"Streaming reduces perceived latency"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Audio Playback"}),(0,i.jsx)(n.td,{children:"~10ms"}),(0,i.jsx)(n.td,{children:"Browser processing"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Total typical latency: 800ms - 4000ms"})}),"\n",(0,i.jsx)(n.h3,{id:"latency-optimization",children:"Latency Optimization"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Streaming TTS"}),": Start speaking before full response is generated"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Local STT"}),": Use Sherpa-ONNX instead of cloud APIs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Edge deployment"}),": Run agents closer to users"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Response caching"}),": Cache common responses"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/architecture/session-lifecycle",children:"Session Lifecycle"})," - Session states"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/architecture/kubernetes-orchestration",children:"Kubernetes Orchestration"})," - Pod management"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>a,x:()=>l});var t=s(6540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);