"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[7854],{1767(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"agent-sdk/audio-pipeline","title":"\ud83c\udf99\ufe0f Audio Pipeline","description":"The Audio Pipeline manages the flow of audio through your agent, handling speech-to-text (STT) and text-to-speech (TTS) conversion.","source":"@site/docs/agent-sdk/audio-pipeline.md","sourceDirName":"agent-sdk","slug":"/agent-sdk/audio-pipeline","permalink":"/STELLA_backend/docs/agent-sdk/audio-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/c4dhi/STELLA_backend/tree/main/docs-site/docs/agent-sdk/audio-pipeline.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"\ud83c\udf99\ufe0f Audio Pipeline"}}');var t=i(4848),l=i(8453);const r={sidebar_position:5,title:"\ud83c\udf99\ufe0f Audio Pipeline"},a="\ud83c\udf99\ufe0f Audio Pipeline",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Basic Usage",id:"basic-usage",level:2},{value:"STT Providers",id:"stt-providers",level:2},{value:"Sherpa (Default)",id:"sherpa-default",level:3},{value:"Whisper (OpenAI)",id:"whisper-openai",level:3},{value:"Deepgram",id:"deepgram",level:3},{value:"TTS Providers",id:"tts-providers",level:2},{value:"Kokoro (Default)",id:"kokoro-default",level:3},{value:"ElevenLabs",id:"elevenlabs",level:3},{value:"OpenAI TTS",id:"openai-tts",level:3},{value:"Pipeline Methods",id:"pipeline-methods",level:2},{value:"speech_to_text",id:"speech_to_text",level:3},{value:"text_to_speech",id:"text_to_speech",level:3},{value:"Voice Activity Detection (VAD)",id:"voice-activity-detection-vad",level:2},{value:"Interruption Handling",id:"interruption-handling",level:2},{value:"Audio Format",id:"audio-format",level:2},{value:"Complete Example",id:"complete-example",level:2},{value:"See Also",id:"see-also",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"\ufe0f-audio-pipeline",children:"\ud83c\udf99\ufe0f Audio Pipeline"})}),"\n",(0,t.jsx)(n.p,{children:"The Audio Pipeline manages the flow of audio through your agent, handling speech-to-text (STT) and text-to-speech (TTS) conversion."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Audio Input \u2192 STT \u2192 Text Processing \u2192 TTS \u2192 Audio Output\n"})}),"\n",(0,t.jsx)(n.p,{children:"The pipeline handles:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Receiving audio from LiveKit"}),"\n",(0,t.jsx)(n.li,{children:"Converting speech to text (STT)"}),"\n",(0,t.jsx)(n.li,{children:"Sending text to your agent for processing"}),"\n",(0,t.jsx)(n.li,{children:"Converting responses to speech (TTS)"}),"\n",(0,t.jsx)(n.li,{children:"Publishing audio back to LiveKit"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from stella_sdk import AudioPipeline\n\n# Create pipeline with defaults\npipeline = AudioPipeline()\n\n# Or configure providers\npipeline = AudioPipeline(\n    stt_provider="sherpa",\n    tts_provider="kokoro"\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"stt-providers",children:"STT Providers"}),"\n",(0,t.jsx)(n.h3,{id:"sherpa-default",children:"Sherpa (Default)"}),"\n",(0,t.jsx)(n.p,{children:"Local, open-source speech recognition:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pipeline = AudioPipeline(\n    stt_provider="sherpa",\n    stt_config={\n        "model": "sherpa-onnx-whisper-tiny.en",\n        "sample_rate": 16000\n    }\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"No API costs"}),"\n",(0,t.jsx)(n.li,{children:"Low latency"}),"\n",(0,t.jsx)(n.li,{children:"Privacy (runs locally)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Requires more CPU"}),"\n",(0,t.jsx)(n.li,{children:"Less accurate than cloud services"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"whisper-openai",children:"Whisper (OpenAI)"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI's Whisper API:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pipeline = AudioPipeline(\n    stt_provider="whisper",\n    stt_config={\n        "model": "whisper-1",\n        "language": "en"\n    }\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"High accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Multi-language support"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Requires API key"}),"\n",(0,t.jsx)(n.li,{children:"Per-usage cost"}),"\n",(0,t.jsx)(n.li,{children:"Network latency"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"deepgram",children:"Deepgram"}),"\n",(0,t.jsx)(n.p,{children:"Deepgram's real-time transcription:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pipeline = AudioPipeline(\n    stt_provider="deepgram",\n    stt_config={\n        "model": "nova-2",\n        "language": "en-US",\n        "smart_format": True\n    }\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Real-time streaming"}),"\n",(0,t.jsx)(n.li,{children:"High accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Speaker diarization"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"tts-providers",children:"TTS Providers"}),"\n",(0,t.jsx)(n.h3,{id:"kokoro-default",children:"Kokoro (Default)"}),"\n",(0,t.jsx)(n.p,{children:"Local TTS using Kokoro:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pipeline = AudioPipeline(\n    tts_provider="kokoro",\n    tts_config={\n        "voice": "af_heart",\n        "speed": 1.0\n    }\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"No API costs"}),"\n",(0,t.jsx)(n.li,{children:"Low latency"}),"\n",(0,t.jsx)(n.li,{children:"Privacy"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Limited voices"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"elevenlabs",children:"ElevenLabs"}),"\n",(0,t.jsx)(n.p,{children:"Premium voice synthesis:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pipeline = AudioPipeline(\n    tts_provider="elevenlabs",\n    tts_config={\n        "voice_id": "21m00Tcm4TlvDq8ikWAM",\n        "model_id": "eleven_turbo_v2",\n        "stability": 0.5,\n        "similarity_boost": 0.75\n    }\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"High-quality voices"}),"\n",(0,t.jsx)(n.li,{children:"Voice cloning"}),"\n",(0,t.jsx)(n.li,{children:"Emotional range"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Per-character cost"}),"\n",(0,t.jsx)(n.li,{children:"Requires API key"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"openai-tts",children:"OpenAI TTS"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI's text-to-speech:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pipeline = AudioPipeline(\n    tts_provider="openai",\n    tts_config={\n        "model": "tts-1",\n        "voice": "alloy"  # alloy, echo, fable, onyx, nova, shimmer\n    }\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"pipeline-methods",children:"Pipeline Methods"}),"\n",(0,t.jsx)(n.h3,{id:"speech_to_text",children:"speech_to_text"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Transcribe audio buffer\ntranscript = await pipeline.speech_to_text(audio_bytes)\n\n# With streaming callback\nasync def on_partial(text: str, is_final: bool):\n    print(f\"{'Final' if is_final else 'Partial'}: {text}\")\n\nawait pipeline.speech_to_text(\n    audio_bytes,\n    callback=on_partial\n)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"text_to_speech",children:"text_to_speech"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Generate audio from text\naudio = await pipeline.text_to_speech("Hello, world!")\n\n# Streaming TTS\nasync for audio_chunk in pipeline.text_to_speech_stream("Hello, world!"):\n    await agent.publish_audio(audio_chunk)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"voice-activity-detection-vad",children:"Voice Activity Detection (VAD)"}),"\n",(0,t.jsx)(n.p,{children:"The pipeline includes VAD to detect when someone is speaking:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from stella_sdk import AudioPipeline, VADConfig\n\npipeline = AudioPipeline(\n    vad_config=VADConfig(\n        threshold=0.5,        # Detection sensitivity (0-1)\n        min_speech_ms=250,    # Minimum speech duration\n        min_silence_ms=500,   # Silence before end of speech\n        sample_rate=16000\n    )\n)\n\n# VAD events\npipeline.on_speech_start = lambda: print("Speech started")\npipeline.on_speech_end = lambda audio: print(f"Speech ended: {len(audio)} bytes")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"interruption-handling",children:"Interruption Handling"}),"\n",(0,t.jsx)(n.p,{children:"Handle when the user interrupts the agent:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MyAgent(BaseAgent):\n    async def on_transcript(self, text: str, is_final: bool):\n        if not is_final:\n            # User is speaking - stop agent audio\n            await self.stop_speaking()\n            return\n\n        # Process final transcript\n        response = await self.generate_response(text)\n        await self.speak(response)\n\n    async def stop_speaking(self):\n        """Stop any currently playing agent audio."""\n        await self.pipeline.cancel_tts()\n        await self.send_status("listening")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"audio-format",children:"Audio Format"}),"\n",(0,t.jsx)(n.p,{children:"Default audio format:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Property"}),(0,t.jsx)(n.th,{children:"Value"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Sample Rate"}),(0,t.jsx)(n.td,{children:"16000 Hz"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Channels"}),(0,t.jsx)(n.td,{children:"1 (mono)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Bit Depth"}),(0,t.jsx)(n.td,{children:"16-bit"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Format"}),(0,t.jsx)(n.td,{children:"PCM"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"Configure in the pipeline:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pipeline = AudioPipeline(\n    sample_rate=16000,\n    channels=1,\n    bit_depth=16\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"complete-example",children:"Complete Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from stella_sdk import BaseAgent, AudioPipeline, VADConfig\n\nclass VoiceAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.pipeline = AudioPipeline(\n            stt_provider="sherpa",\n            tts_provider="kokoro",\n            vad_config=VADConfig(\n                threshold=0.5,\n                min_silence_ms=700\n            )\n        )\n        self.is_speaking = False\n\n    async def on_connect(self):\n        # Start listening for audio\n        self.pipeline.on_speech_end = self.on_speech_detected\n        await self.send_status("listening")\n\n    async def on_speech_detected(self, audio: bytes):\n        """Called when VAD detects end of speech."""\n        if self.is_speaking:\n            # User interrupted - stop speaking\n            await self.pipeline.cancel_tts()\n            self.is_speaking = False\n\n        # Transcribe\n        transcript = await self.pipeline.speech_to_text(audio)\n        await self.send_transcript(transcript, speaker="user")\n\n        # Generate response\n        await self.send_status("thinking")\n        response = await self.generate_response(transcript)\n\n        # Speak response\n        await self.send_status("speaking")\n        self.is_speaking = True\n\n        await self.send_transcript(response, speaker="assistant")\n\n        async for chunk in self.pipeline.text_to_speech_stream(response):\n            if not self.is_speaking:\n                break  # Interrupted\n            await self.publish_audio(chunk)\n\n        self.is_speaking = False\n        await self.send_status("listening")\n\n    async def generate_response(self, text: str) -> str:\n        # Your LLM logic\n        pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/agent-sdk/base-agent",children:"Base Agent"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/agent-sdk/message-types",children:"Message Types"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/docs/agent-sdk/building-custom-agent",children:"Building Custom Agents"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var s=i(6540);const t={},l=s.createContext(t);function r(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);