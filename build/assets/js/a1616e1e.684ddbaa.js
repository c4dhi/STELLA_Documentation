"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[2743],{2954(e,n,t){t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"sdk/streaming","title":"Streaming","description":"Real-time audio streaming in STELLA agents","source":"@site/docs/sdk/streaming.md","sourceDirName":"sdk","slug":"/sdk/streaming","permalink":"/STELLA_backend/docs/sdk/streaming","draft":false,"unlisted":false,"editUrl":"https://github.com/c4dhi/STELLA_backend/tree/main/docs-site/docs/sdk/streaming.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Streaming","description":"Real-time audio streaming in STELLA agents"},"sidebar":"docsSidebar","previous":{"title":"Tools","permalink":"/STELLA_backend/docs/sdk/tools"},"next":{"title":"TypeScript Types","permalink":"/STELLA_backend/docs/sdk/typescript-types"}}');var i=t(4848),a=t(8453);const r={sidebar_position:5,title:"Streaming",description:"Real-time audio streaming in STELLA agents"},l="Streaming",o={},c=[{value:"Audio Pipeline",id:"audio-pipeline",level:2},{value:"Input Streaming (STT)",id:"input-streaming-stt",level:2},{value:"Continuous Recognition",id:"continuous-recognition",level:3},{value:"Voice Activity Detection (VAD)",id:"voice-activity-detection-vad",level:3},{value:"Audio Frame Processing",id:"audio-frame-processing",level:3},{value:"Output Streaming (TTS)",id:"output-streaming-tts",level:2},{value:"Streaming TTS",id:"streaming-tts",level:3},{value:"Sentence-by-Sentence Streaming",id:"sentence-by-sentence-streaming",level:3},{value:"LLM Streaming with TTS",id:"llm-streaming-with-tts",level:3},{value:"Audio Formats",id:"audio-formats",level:2},{value:"Input Audio",id:"input-audio",level:3},{value:"Output Audio",id:"output-audio",level:3},{value:"Interruption Handling",id:"interruption-handling",level:2},{value:"Latency Optimization",id:"latency-optimization",level:2},{value:"Reduce STT Latency",id:"reduce-stt-latency",level:3},{value:"Reduce TTS Latency",id:"reduce-tts-latency",level:3},{value:"Prefetch Common Responses",id:"prefetch-common-responses",level:3},{value:"Metrics and Monitoring",id:"metrics-and-monitoring",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"streaming",children:"Streaming"})}),"\n",(0,i.jsx)(n.p,{children:"STELLA agents work with real-time audio streams for both input (STT) and output (TTS). This guide covers audio handling, streaming patterns, and optimization."}),"\n",(0,i.jsx)(n.h2,{id:"audio-pipeline",children:"Audio Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"User Microphone \u2500\u2500\u25b6 WebRTC \u2500\u2500\u25b6 Agent \u2500\u2500\u25b6 STT \u2500\u2500\u25b6 Text\n                                                    \u2502\n                                                    \u25bc\nUser Speakers \u25c0\u2500\u2500 WebRTC \u25c0\u2500\u2500 Agent \u25c0\u2500\u2500 TTS \u25c0\u2500\u2500 LLM Response\n"})}),"\n",(0,i.jsx)(n.h2,{id:"input-streaming-stt",children:"Input Streaming (STT)"}),"\n",(0,i.jsx)(n.h3,{id:"continuous-recognition",children:"Continuous Recognition"}),"\n",(0,i.jsx)(n.p,{children:"The AudioPipeline processes incoming audio continuously:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from stella_sdk import BaseAgent, AudioPipeline\n\n\nclass MyAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.pipeline = AudioPipeline(\n            stt_provider="sherpa",\n            stt_model="sherpa-onnx-streaming-zipformer"\n        )\n\n    async def on_transcript(self, text: str, is_final: bool):\n        # Interim results (while user is speaking)\n        if not is_final:\n            await self.send_transcript(text, speaker="user", is_final=False)\n            return\n\n        # Final result (user finished speaking)\n        await self.send_transcript(text, speaker="user", is_final=True)\n        await self.process_input(text)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"voice-activity-detection-vad",children:"Voice Activity Detection (VAD)"}),"\n",(0,i.jsx)(n.p,{children:"VAD determines when the user starts and stops speaking:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'self.pipeline = AudioPipeline(\n    stt_provider="sherpa",\n    vad_enabled=True,\n    vad_threshold=0.5,         # Sensitivity (0-1)\n    vad_min_silence_ms=500,    # Silence to end utterance\n    vad_min_speech_ms=250      # Minimum speech duration\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"audio-frame-processing",children:"Audio Frame Processing"}),"\n",(0,i.jsx)(n.p,{children:"For advanced use cases, access raw audio frames:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def on_audio_frame(self, frame: AudioFrame):\n    """Process raw audio frames."""\n    # frame.data: bytes (PCM audio)\n    # frame.sample_rate: int (usually 48000)\n    # frame.num_channels: int (usually 1)\n    # frame.samples_per_channel: int\n\n    # Custom processing\n    processed = self.custom_processor(frame.data)\n\n    # Or pass to custom STT\n    self.stt_buffer.append(frame.data)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"output-streaming-tts",children:"Output Streaming (TTS)"}),"\n",(0,i.jsx)(n.h3,{id:"streaming-tts",children:"Streaming TTS"}),"\n",(0,i.jsx)(n.p,{children:"Generate and play audio as it's produced:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def speak(self, text: str):\n    """Stream TTS output."""\n    # Notify frontend\n    await self.send_status("speaking")\n    await self.send_transcript(text, speaker="assistant")\n\n    # Stream audio chunks as they\'re generated\n    async for chunk in self.pipeline.text_to_speech_stream(text):\n        await self.publish_audio(chunk)\n\n    # Done speaking\n    await self.send_status("listening")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"sentence-by-sentence-streaming",children:"Sentence-by-Sentence Streaming"}),"\n",(0,i.jsx)(n.p,{children:"For long responses, stream sentence by sentence:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def speak_sentences(self, text: str):\n    """Stream TTS by sentence for lower latency."""\n    sentences = self.split_sentences(text)\n\n    for i, sentence in enumerate(sentences):\n        # Start TTS for this sentence\n        async for chunk in self.pipeline.text_to_speech_stream(sentence):\n            await self.publish_audio(chunk)\n\n        # Small pause between sentences\n        if i < len(sentences) - 1:\n            await asyncio.sleep(0.1)\n\ndef split_sentences(self, text: str) -> list[str]:\n    """Split text into sentences."""\n    import re\n    return re.split(r\'(?<=[.!?])\\s+\', text)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"llm-streaming-with-tts",children:"LLM Streaming with TTS"}),"\n",(0,i.jsx)(n.p,{children:"Stream LLM output directly to TTS:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def stream_response(self, user_input: str):\n    """Stream LLM response with real-time TTS."""\n    await self.send_status("thinking")\n\n    # Buffer for collecting text\n    text_buffer = ""\n    full_response = ""\n\n    # Stream from LLM\n    stream = await self.openai.chat.completions.create(\n        model="gpt-4o",\n        messages=[\n            {"role": "system", "content": self.system_prompt},\n            {"role": "user", "content": user_input}\n        ],\n        stream=True\n    )\n\n    await self.send_status("speaking")\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            text = chunk.choices[0].delta.content\n            text_buffer += text\n            full_response += text\n\n            # Check for sentence boundary\n            if self.has_sentence_end(text_buffer):\n                sentence, text_buffer = self.extract_sentence(text_buffer)\n\n                # Stream TTS for the sentence\n                async for audio in self.pipeline.text_to_speech_stream(sentence):\n                    await self.publish_audio(audio)\n\n    # Handle remaining text\n    if text_buffer.strip():\n        async for audio in self.pipeline.text_to_speech_stream(text_buffer):\n            await self.publish_audio(audio)\n\n    # Send final transcript\n    await self.send_transcript(full_response, speaker="assistant")\n    await self.send_status("listening")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"audio-formats",children:"Audio Formats"}),"\n",(0,i.jsx)(n.h3,{id:"input-audio",children:"Input Audio"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Property"}),(0,i.jsx)(n.th,{children:"Value"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Codec"}),(0,i.jsx)(n.td,{children:"Opus"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Sample Rate"}),(0,i.jsx)(n.td,{children:"48000 Hz"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Channels"}),(0,i.jsx)(n.td,{children:"Mono (1)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Bit Depth"}),(0,i.jsx)(n.td,{children:"16-bit"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"output-audio",children:"Output Audio"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Configure TTS output format\nself.pipeline = AudioPipeline(\n    tts_provider="kokoro",\n    tts_sample_rate=24000,   # Sample rate\n    tts_format="pcm_s16le",  # 16-bit PCM\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"interruption-handling",children:"Interruption Handling"}),"\n",(0,i.jsx)(n.p,{children:"Handle user interruption (barge-in):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MyAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.is_speaking = False\n        self.tts_task = None\n\n    async def speak(self, text: str):\n        self.is_speaking = True\n        self.tts_task = asyncio.current_task()\n\n        try:\n            async for chunk in self.pipeline.text_to_speech_stream(text):\n                await self.publish_audio(chunk)\n        except asyncio.CancelledError:\n            # Interrupted\n            pass\n        finally:\n            self.is_speaking = False\n            self.tts_task = None\n\n    async def on_data_message(self, message: dict):\n        if message.get("type") == "control":\n            if message["data"]["action"] == "interrupt":\n                await self.handle_interrupt()\n\n    async def handle_interrupt(self):\n        """Handle user interruption."""\n        if self.is_speaking and self.tts_task:\n            self.tts_task.cancel()\n            await self.pipeline.cancel_tts()\n            await self.send_status("listening")\n\n    async def on_transcript(self, text: str, is_final: bool):\n        # Auto-interrupt when user starts speaking\n        if self.is_speaking and is_final:\n            await self.handle_interrupt()\n\n        # Process input\n        if is_final:\n            await self.process_input(text)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"latency-optimization",children:"Latency Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"reduce-stt-latency",children:"Reduce STT Latency"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Use streaming STT model\nself.pipeline = AudioPipeline(\n    stt_provider="sherpa",\n    stt_model="sherpa-onnx-streaming-zipformer",  # Streaming model\n    stt_chunk_size=160,  # Smaller chunks = lower latency\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"reduce-tts-latency",children:"Reduce TTS Latency"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Start speaking before LLM completes\nasync def low_latency_response(self, user_input: str):\n    # Start with acknowledgment\n    await self.speak_quick("Let me check that for you.")\n\n    # Then generate full response\n    response = await self.generate_response(user_input)\n    await self.speak(response)\n\nasync def speak_quick(self, text: str):\n    """Quick acknowledgment with minimal latency."""\n    # Use faster TTS settings\n    async for chunk in self.pipeline.text_to_speech_stream(\n        text,\n        speed=1.1  # Slightly faster\n    ):\n        await self.publish_audio(chunk)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"prefetch-common-responses",children:"Prefetch Common Responses"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MyAgent(BaseAgent):\n    def __init__(self):\n        super().__init__()\n        self.audio_cache = {}\n\n    async def prefetch_audio(self):\n        """Pre-generate common responses."""\n        common_phrases = [\n            "I understand. Let me help you with that.",\n            "Could you please clarify?",\n            "One moment please.",\n        ]\n\n        for phrase in common_phrases:\n            audio_data = await self.pipeline.text_to_speech(phrase)\n            self.audio_cache[phrase] = audio_data\n\n    async def speak_cached(self, text: str):\n        """Use cached audio if available."""\n        if text in self.audio_cache:\n            await self.publish_audio(self.audio_cache[text])\n        else:\n            await self.speak(text)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"metrics-and-monitoring",children:"Metrics and Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import time\n\nclass MyAgent(BaseAgent):\n    async def on_transcript(self, text: str, is_final: bool):\n        if is_final:\n            start_time = time.time()\n\n            # Generate response\n            response = await self.generate_response(text)\n\n            llm_latency = time.time() - start_time\n\n            # Speak response\n            tts_start = time.time()\n            await self.speak(response)\n            tts_latency = time.time() - tts_start\n\n            # Log metrics\n            self.logger.info(\n                "Response metrics",\n                llm_latency_ms=llm_latency * 1000,\n                tts_latency_ms=tts_latency * 1000,\n                response_length=len(response)\n            )\n'})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/sdk/base-agent",children:"Base Agent"})," - Full API reference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/sdk/tools",children:"Tools"})," - Building custom tools"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/architecture/data-flow",children:"Data Flow"})," - System data flow"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>l});var s=t(6540);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);